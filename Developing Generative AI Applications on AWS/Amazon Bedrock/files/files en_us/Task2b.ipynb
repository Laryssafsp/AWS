{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fded102b",
   "metadata": {},
   "source": [
    "# Task 2b: Abstractive Text Summarization\n",
    "\n",
    "In this notebook, you manage challenges arising in large document summarization - input text can exceed model context lengths, generate hallucinated outputs, or trigger out-of-memory errors.\n",
    "\n",
    "To mitigate these issues, this notebook demonstrates an architecture using prompt chunking and chaining with the [LangChain](https://python.langchain.com/docs/get_started/introduction.html) framework, a toolkit enabling applications leveraging language models.\n",
    "\n",
    "You explore an approach addressing scenarios when user documents surpass token limits. Chunking splits documents into segments under context length thresholds before sequentially feeding them to models. This chains prompts across chunks, retaining prior context. You apply this approach to summarize call transcripts, meetings transcripts, books, articles, blog posts, and other relevant content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c1eaf9",
   "metadata": {},
   "source": [
    "## Task 2b.1: Environment setup\n",
    "\n",
    "In this task, you set up your environment and create a Bedrock client that automatically detects your AWS region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f0f9067",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T18:15:25.304993Z",
     "iopub.status.busy": "2025-08-11T18:15:25.304679Z",
     "iopub.status.idle": "2025-08-11T18:15:25.590460Z",
     "shell.execute_reply": "2025-08-11T18:15:25.589575Z",
     "shell.execute_reply.started": "2025-08-11T18:15:25.304973Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a service client by name using the default session.\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "# AWS and Bedrock imports\n",
    "import boto3\n",
    "\n",
    "# Get the region programmatically\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name or \"us-east-1\"  # Default to us-east-1 if region not set\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name=region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae9a41",
   "metadata": {},
   "source": [
    "## Task 2b.2: Summarize long text \n",
    "\n",
    "### Configuring LangChain with Boto3\n",
    "\n",
    "In this task, you specify the LLM for the LangChain Bedrock class, and pass arguments for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93df2442",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T18:15:25.595945Z",
     "iopub.status.busy": "2025-08-11T18:15:25.595494Z",
     "iopub.status.idle": "2025-08-11T18:15:26.672177Z",
     "shell.execute_reply": "2025-08-11T18:15:26.671460Z",
     "shell.execute_reply.started": "2025-08-11T18:15:25.595914Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LangChain imports\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "# Base LLM configuration\n",
    "modelId = \"amazon.nova-lite-v1:0\"\n",
    "\n",
    "class NovaLiteWrapper(LLM):\n",
    "    \"\"\"Wrapper for Nova Lite model that formats inputs correctly.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"nova-lite-wrapper\"\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Format prompt for Nova Lite and process.\"\"\"\n",
    "        # Format the prompt for Nova Lite's expected message structure\n",
    "        formatted_input = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"text\": prompt}]  # Content must be an array with text objects\n",
    "                }\n",
    "            ],\n",
    "            \"inferenceConfig\": {\n",
    "                \"maxTokens\": 2048,\n",
    "                \"temperature\": 0,\n",
    "                \"topP\": 0.9\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Call Bedrock directly with the properly formatted input\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=modelId,\n",
    "            body=json.dumps(formatted_input)\n",
    "        )\n",
    "        \n",
    "        # Parse the response - updated to handle Nova Lite's response format\n",
    "        response_body = json.loads(response['body'].read().decode('utf-8'))\n",
    "        \n",
    "        # Extract the text from the response\n",
    "        if 'output' in response_body and 'message' in response_body['output']:\n",
    "            message = response_body['output']['message']\n",
    "            if 'content' in message and isinstance(message['content'], list):\n",
    "                # Extract text from each content item\n",
    "                texts = []\n",
    "                for content_item in message['content']:\n",
    "                    if isinstance(content_item, dict) and 'text' in content_item:\n",
    "                        texts.append(content_item['text'])\n",
    "                return ' '.join(texts)\n",
    "        \n",
    "        # Fallback if the response format is different\n",
    "        return str(response_body)\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"model_id\": modelId}\n",
    "    \n",
    "    def get_num_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate token count - Nova Lite uses roughly 1 token per 4 characters.\"\"\"\n",
    "        return len(text) // 4  # Rough approximation\n",
    "\n",
    "# Create the Nova Lite wrapper\n",
    "llm = NovaLiteWrapper()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d423aa-cb67-4170-afdb-b037f2531921",
   "metadata": {},
   "source": [
    "## Creating a Resource-Optimized LLM Wrapper\n",
    "\n",
    "To handle Bedrock service quotas effectively, we'll create a wrapper class that optimizes resource usage and implements exponential backoff with jitter for API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a87119b-bf9d-4bec-be54-1efbcc9e8edf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T18:15:26.676115Z",
     "iopub.status.busy": "2025-08-11T18:15:26.675667Z",
     "iopub.status.idle": "2025-08-11T18:15:26.770570Z",
     "shell.execute_reply": "2025-08-11T18:15:26.769805Z",
     "shell.execute_reply.started": "2025-08-11T18:15:26.676084Z"
    }
   },
   "outputs": [],
   "source": [
    "# Enhanced resource-optimized LLM wrapper with exponential backoff\n",
    "class ResourceOptimizedLLM(LLM):\n",
    "    \"\"\"Wrapper that optimizes resource usage for LLM processing.\"\"\"\n",
    "    \n",
    "    llm: Any  # The base LLM to wrap\n",
    "    min_pause: float = 30.0  # Minimum pause between requests\n",
    "    max_pause: float = 60.0  # Maximum pause after throttling\n",
    "    initial_pause: float = 10.0  # Initial pause between requests\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return f\"optimized-{self.llm._llm_type}\"\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Process with resource optimization and exponential backoff.\"\"\"\n",
    "        # Always pause between requests to optimize resource usage\n",
    "        time.sleep(self.initial_pause)\n",
    "        \n",
    "        # Implement retry with exponential backoff\n",
    "        max_retries = 10  # More retries for important operations\n",
    "        base_delay = self.min_pause\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"Making API call (attempt {attempt+1}/{max_retries})...\")\n",
    "                return self.llm._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                \n",
    "                # Handle different types of service exceptions\n",
    "                if any(err in error_str for err in [\"ThrottlingException\", \"TooManyRequests\", \"Rate exceeded\"]):\n",
    "                    if attempt < max_retries - 1:\n",
    "                        # Calculate backoff with jitter to prevent request clustering\n",
    "                        jitter = random.random() * 0.5\n",
    "                        wait_time = min(base_delay * (2 ** attempt) + jitter, self.max_pause)\n",
    "                        \n",
    "                        print(f\"Service capacity reached. Backing off for {wait_time:.2f} seconds...\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(\"Maximum retries reached. Consider reducing batch size or increasing delays.\")\n",
    "                        raise\n",
    "                else:\n",
    "                    # For non-capacity errors, don't retry\n",
    "                    print(f\"Non-capacity error: {error_str}\")\n",
    "                    raise\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {**self.llm._identifying_params, \"initial_pause\": self.initial_pause}\n",
    "    \n",
    "    def get_num_tokens(self, text: str) -> int:\n",
    "        \"\"\"Pass through token counting to the base model.\"\"\"\n",
    "        return self.llm.get_num_tokens(text)\n",
    "\n",
    "# Create the resource-optimized LLM\n",
    "resource_optimized_llm = ResourceOptimizedLLM(llm=llm, initial_pause=10.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b8886-b2e9-4239-8556-1f1483e9bfef",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** This wrapper adds important features for production use:\n",
    "\n",
    "- Automatic pausing between requests to respect service quotas\n",
    "- Exponential backoff with jitter for handling throttling exceptions\n",
    "- Comprehensive error handling and reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31223056",
   "metadata": {},
   "source": [
    "## Task 2b.3: Loading a text file with many tokens\n",
    "\n",
    "In this task, you use a copy of [Amazon's CEO letter to shareholders in 2022](https://www.aboutamazon.com/news/company-news/amazon-ceo-andy-jassy-2022-letter-to-shareholders) in the letters directory. You create a function to load the text file and handle potential errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c70352ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T18:15:26.777794Z",
     "iopub.status.busy": "2025-08-11T18:15:26.777481Z",
     "iopub.status.idle": "2025-08-11T18:15:26.784181Z",
     "shell.execute_reply": "2025-08-11T18:15:26.783534Z",
     "shell.execute_reply.started": "2025-08-11T18:15:26.777765Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded successfully with 8109 tokens\n"
     ]
    }
   ],
   "source": [
    "# Document loading function\n",
    "def load_document(file_path):\n",
    "    \"\"\"Load document from file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading document: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "shareholder_letter = \"../letters/2022-letter.txt\"\n",
    "letter = load_document(shareholder_letter)\n",
    "\n",
    "if letter:\n",
    "    num_tokens = resource_optimized_llm.get_num_tokens(letter)\n",
    "    print(f\"Document loaded successfully with {num_tokens} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0e622",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** You can safely ignore the warnings and proceed to next cell. We'll address this by chunking the document in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ec39d",
   "metadata": {},
   "source": [
    "## Task 2b.4: Splitting the long text into chunks\n",
    "\n",
    "In this task, you split the text into smaller chunks because it is too long to fit in the prompt. `RecursiveCharacterTextSplitter` in LangChain supports splitting long text into chunks recursively until the size of each chunk becomes smaller than chunk_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e7c372b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T18:15:26.790908Z",
     "iopub.status.busy": "2025-08-11T18:15:26.790551Z",
     "iopub.status.idle": "2025-08-11T18:15:26.797604Z",
     "shell.execute_reply": "2025-08-11T18:15:26.796764Z",
     "shell.execute_reply.started": "2025-08-11T18:15:26.790876Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 10 chunks\n",
      "Now we have 10 documents and the first one has 543 tokens\n"
     ]
    }
   ],
   "source": [
    "# Document chunking with conservative settings\n",
    "def chunk_document(text, chunk_size=4000, chunk_overlap=200):\n",
    "    \"\"\"Split document into manageable chunks.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.create_documents([text])\n",
    "    print(f\"Document split into {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "# Split the document into chunks\n",
    "if letter:\n",
    "    docs = chunk_document(letter, chunk_size=4000, chunk_overlap=200)\n",
    "    \n",
    "    if docs:\n",
    "        num_docs = len(docs)\n",
    "        num_tokens_first_doc = resource_optimized_llm.get_num_tokens(docs[0].page_content)\n",
    "        print(f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acedb37-52f7-40ac-ae70-cede47b270a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** The `chunk_size` parameter controls how large each chunk will be. Larger chunks provide more context but require more processing resources. The `chunk_overlap` parameter ensures some continuity between chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f8ae45",
   "metadata": {},
   "source": [
    "## Task 2b.5: Summarizing chunks and combining them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d49f5",
   "metadata": {},
   "source": [
    "In this task, you implement two approaches for summarizing chunked documents: using LangChain's built-in summarize chain and a custom manual implementation that provides better control over resource usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc48123-d9f4-4275-a509-4ed0a687d547",
   "metadata": {},
   "source": [
    "## Understanding Implementation Approaches\n",
    "\n",
    "This notebook demonstrates two different approaches for summarizing large documents with AWS Bedrock:\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** We've included both a standard LangChain implementation and a custom implementation to show the tradeoffs between convenience and control when building production applications.\n",
    "\n",
    "### Two Paths to the Same Goal\n",
    "\n",
    "1. **Standard LangChain Implementation** (`process_documents_with_pacing`):\n",
    "   - Uses LangChain's built-in summarization chains\n",
    "   - Requires less code and is easier to implement\n",
    "   - Abstracts away the underlying complexity\n",
    "   - Great for rapid prototyping and simple use cases\n",
    "\n",
    "2. **Custom Refine Implementation** (`manual_refine_with_optimization`):\n",
    "   - Builds the refinement process step-by-step\n",
    "   - Provides complete visibility into prompts and processing\n",
    "   - Offers granular error handling for each document chunk\n",
    "   - Allows precise control over API call timing and retry logic\n",
    "\n",
    "While both achieve the same end result, the custom implementation gives you more control over the entire process, which is crucial when working with service quotas and building production-ready applications.\n",
    "\n",
    "In real-world scenarios, you might start with the standard implementation during development and then move to a custom implementation when you need more control over resource usage, error handling, or prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a2fa5-a989-4330-ad46-72ddc631b7b8",
   "metadata": {},
   "source": [
    "### Standard LangChain Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e56f7535-31cf-4809-ba1b-a7ef5e3196a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T18:15:26.812634Z",
     "iopub.status.busy": "2025-08-11T18:15:26.812155Z",
     "iopub.status.idle": "2025-08-11T18:15:26.817979Z",
     "shell.execute_reply": "2025-08-11T18:15:26.817208Z",
     "shell.execute_reply.started": "2025-08-11T18:15:26.812603Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom document processing with controlled pacing\n",
    "def process_documents_with_pacing(docs, chain_type=\"refine\", verbose=True):\n",
    "    \"\"\"Process documents with pacing to optimize resource usage.\"\"\"\n",
    "    \n",
    "    # Configure the chain\n",
    "    summary_chain = load_summarize_chain(\n",
    "        llm=resource_optimized_llm,\n",
    "        chain_type=chain_type,  # \"refine\" processes sequentially, good for resource optimization\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Process with additional error handling\n",
    "    try:\n",
    "        result = summary_chain.invoke(docs)\n",
    "        return result\n",
    "    except ValueError as error:\n",
    "        if \"AccessDeniedException\" in str(error):\n",
    "            print(f\"\\n\\033[91mAccess Denied: {error}\\033[0m\")\n",
    "            print(\"\\nTo troubleshoot this issue, please check:\")\n",
    "            print(\"1. Your IAM permissions for Bedrock\")\n",
    "            print(\"2. Model access permissions\")\n",
    "            print(\"3. AWS credentials configuration\")\n",
    "            return {\"output_text\": \"Error: Access denied. Check permissions.\"}\n",
    "        else:\n",
    "            print(f\"\\n\\033[91mError during processing: {error}\\033[0m\")\n",
    "            return {\"output_text\": f\"Error during processing: {str(error)}\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988cc73-0982-4b69-88cb-7066176d29b5",
   "metadata": {},
   "source": [
    "### Custom Refine Implementation with Enhanced Resource Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9216651f-0fc4-42e6-93c8-8adf65b729a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T18:15:26.823183Z",
     "iopub.status.busy": "2025-08-11T18:15:26.822800Z",
     "iopub.status.idle": "2025-08-11T18:15:26.830164Z",
     "shell.execute_reply": "2025-08-11T18:15:26.829427Z",
     "shell.execute_reply.started": "2025-08-11T18:15:26.823161Z"
    }
   },
   "outputs": [],
   "source": [
    "# Manual implementation of resource-optimized processing for refine chain\n",
    "def manual_refine_with_optimization(docs, llm, verbose=True):\n",
    "    \"\"\"Manually implement refine chain with resource optimization.\"\"\"\n",
    "    if not docs:\n",
    "        return {\"output_text\": \"No documents to process.\"}\n",
    "    \n",
    "    # Process first document to get initial summary\n",
    "    print(f\"Processing initial document (1/{len(docs)})...\")\n",
    "    \n",
    "    # Simple prompt for initial document\n",
    "    initial_prompt = \"\"\"Write a concise summary of the following:\n",
    "    \"{text}\"\n",
    "    CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "    # Process first document\n",
    "    try:\n",
    "        current_summary = llm(initial_prompt.format(text=docs[0].page_content))\n",
    "        print(\"Initial summary created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating initial summary: {e}\")\n",
    "        return {\"output_text\": \"Failed to create initial summary.\"}\n",
    "    \n",
    "    # Process remaining documents with refine approach\n",
    "    for i, doc in enumerate(docs[1:], start=2):\n",
    "        print(f\"Refining with document {i}/{len(docs)}...\")\n",
    "        \n",
    "        # Refine prompt\n",
    "        refine_prompt = \"\"\"Your job is to refine an existing summary.\n",
    "        We have an existing summary: {existing_summary}\n",
    "        \n",
    "        We have a new document to add information from: {text}\n",
    "        \n",
    "        Please update the summary to incorporate new information from the document.\n",
    "        If the document doesn't contain relevant information, return the existing summary.\n",
    "        \n",
    "        REFINED SUMMARY:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Apply resource optimization between requests\n",
    "            time.sleep(10.0)  # Base delay between requests\n",
    "            \n",
    "            # Update the summary\n",
    "            current_summary = llm(refine_prompt.format(\n",
    "                existing_summary=current_summary,\n",
    "                text=doc.page_content\n",
    "            ))\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Successfully refined with document {i}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during refinement with document {i}: {e}\")\n",
    "            # Apply exponential backoff\n",
    "            backoff = min(10.0 * (2 ** (i % 5)) + (random.random() * 2), 30)\n",
    "            print(f\"Backing off for {backoff:.2f} seconds...\")\n",
    "            time.sleep(backoff)\n",
    "            \n",
    "            # Try one more time\n",
    "            try:\n",
    "                current_summary = llm(refine_prompt.format(\n",
    "                    existing_summary=current_summary,\n",
    "                    text=doc.page_content\n",
    "                ))\n",
    "            except Exception as retry_error:\n",
    "                print(f\"Retry failed for document {i}: {retry_error}\")\n",
    "                # Continue with current summary rather than failing completely\n",
    "    \n",
    "    return {\"output_text\": current_summary}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d2892-54d2-43a1-a69c-58a45432cfe1",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** The manual implementation gives you more control over:\n",
    "\n",
    "- The exact prompts used for summarization\n",
    "- Error handling and recovery\n",
    "- Resource optimization between API calls\n",
    "- Graceful degradation when errors occur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c914f0-cbc3-48b4-a9ee-f14d02ce921e",
   "metadata": {},
   "source": [
    "## Task 2b.6: Main Execution Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e4d11-a876-4bd9-9d91-404017452916",
   "metadata": {},
   "source": [
    "Now we'll create a main function that orchestrates the entire document summarization process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819efba-6a3f-4aaa-8668-13b45993af10",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** The main function (`summarize_document`) allows you to choose which implementation to use based on the `chain_type` parameter, making it easy to compare results and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f3d0718-2adf-4bf9-bf18-acf4644115d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T18:15:26.844563Z",
     "iopub.status.busy": "2025-08-11T18:15:26.844061Z",
     "iopub.status.idle": "2025-08-11T18:15:26.850187Z",
     "shell.execute_reply": "2025-08-11T18:15:26.849508Z",
     "shell.execute_reply.started": "2025-08-11T18:15:26.844532Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main execution function\n",
    "def summarize_document(file_path, chunk_size=4000, chain_type=\"refine\"):\n",
    "    \"\"\"Main function to summarize a document.\"\"\"\n",
    "    \n",
    "    print(f\"Starting document summarization process for: {file_path}\")\n",
    "    \n",
    "    # Load the document\n",
    "    document_text = load_document(file_path)\n",
    "    if not document_text:\n",
    "        return \"Failed to load document.\"\n",
    "    \n",
    "    print(f\"Document loaded successfully. Length: {len(document_text)} characters\")\n",
    "    \n",
    "    # Split into chunks\n",
    "    docs = chunk_document(document_text, chunk_size=chunk_size, chunk_overlap=200)\n",
    "    \n",
    "    # If document is very large, provide a warning\n",
    "    if len(docs) > 15:\n",
    "        print(f\"Warning: Document is large ({len(docs)} chunks). Processing may take some time.\")\n",
    "        \n",
    "        # For very large documents, consider using a subset for testing\n",
    "        if len(docs) > 30:\n",
    "            print(\"Document is extremely large. Consider using a smaller chunk_size or processing a subset.\")\n",
    "            # Optional: process only a subset for testing\n",
    "            # docs = docs[:15]\n",
    "    \n",
    "    # Process the documents\n",
    "    print(f\"Processing document using '{chain_type}' chain type...\")\n",
    "    \n",
    "    # Use the appropriate processing method based on chain type\n",
    "    if chain_type == \"refine\":\n",
    "        # Use our manual implementation for better control over resource optimization\n",
    "        result = manual_refine_with_optimization(docs, resource_optimized_llm)\n",
    "    else:\n",
    "        # Use standard LangChain implementation for other chain types\n",
    "        result = process_documents_with_pacing(docs, chain_type=chain_type)\n",
    "    \n",
    "    # Return the result\n",
    "    if result and \"output_text\" in result:\n",
    "        print(\"\\nSummarization completed successfully!\")\n",
    "        return result[\"output_text\"]\n",
    "    else:\n",
    "        print(\"\\nSummarization failed or returned no result.\")\n",
    "        return \"Summarization process did not produce a valid result.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcfca15-b18e-4aba-8a09-2500fc941f8e",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** Depending on your number of documents, Bedrock request rate quota, and configured retry settings - the summarization process may take some time to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a31741-103d-4524-b3a8-3c68da2baa07",
   "metadata": {},
   "source": [
    "## Task 2b.7: Run the Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0de4dd-9430-46fe-ae9f-8af121f41b51",
   "metadata": {},
   "source": [
    "Let's run the summarization on the shareholder letter. By default, the summarize_document() function uses the refine chain. To enable map_reduce: \n",
    "\n",
    "- Comment out the following line: `summary = summarize_document(document_path, chunk_size=4000, chain_type=\"refine\")`\n",
    "- Uncomment the following line: `# summary = summarize_document(document_path, chunk_size=4000, chain_type=\"map_reduce\")`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff42a88c-44af-4b35-8eed-d8ecd3ea38df",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Note:** Don't worry if you notice error messages during execution. Your code includes robust error handling that will automatically retry failed requests with exponential backoff. This is normal behavior when working with service quotas and demonstrates how production-ready applications should handle API limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2ffd26-bfbd-47d1-b6d7-76daabc8e952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T18:15:26.863998Z",
     "iopub.status.busy": "2025-08-11T18:15:26.863649Z",
     "iopub.status.idle": "2025-08-11T18:19:18.206129Z",
     "shell.execute_reply": "2025-08-11T18:19:18.205423Z",
     "shell.execute_reply.started": "2025-08-11T18:15:26.863976Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3909/1950202955.py:17: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  current_summary = llm(initial_prompt.format(text=docs[0].page_content))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting document summarization process for: ../letters/2022-letter.txt\n",
      "Document loaded successfully. Length: 32438 characters\n",
      "Document split into 10 chunks\n",
      "Processing document using 'refine' chain type...\n",
      "Processing initial document (1/10)...\n",
      "Making API call (attempt 1/10)...\n",
      "Initial summary created successfully.\n",
      "Refining with document 2/10...\n",
      "Making API call (attempt 1/10)...\n",
      "Successfully refined with document 2\n",
      "Refining with document 3/10...\n",
      "Making API call (attempt 1/10)...\n",
      "Successfully refined with document 3\n",
      "Refining with document 4/10...\n",
      "Making API call (attempt 1/10)...\n",
      "Successfully refined with document 4\n",
      "Refining with document 5/10...\n",
      "Making API call (attempt 1/10)...\n",
      "Successfully refined with document 5\n",
      "Refining with document 6/10...\n",
      "Making API call (attempt 1/10)...\n",
      "Successfully refined with document 6\n",
      "Refining with document 7/10...\n",
      "Making API call (attempt 1/10)...\n",
      "Successfully refined with document 7\n",
      "Refining with document 8/10...\n",
      "Making API call (attempt 1/10)...\n",
      "Successfully refined with document 8\n",
      "Refining with document 9/10...\n",
      "Making API call (attempt 1/10)...\n",
      "Successfully refined with document 9\n",
      "Refining with document 10/10...\n",
      "Making API call (attempt 1/10)...\n",
      "Successfully refined with document 10\n",
      "\n",
      "Summarization completed successfully!\n",
      "\n",
      "=== FINAL SUMMARY ===\n",
      "\n",
      "In his second annual shareholder letter, Amazon CEO reflects on the company's resilience and growth despite challenging macroeconomic conditions in 2022. He emphasizes Amazon's ability to innovate and improve customer experience, while making strategic adjustments in investments and future initiatives. The CEO underscores the company's evolution from a small bookstore to a global retail and technology giant, including the development of AWS, Kindle, Alexa, Kuiper, and the new initiative in Large Language Models (LLMs) and Generative AI.\n",
      "\n",
      "The CEO discusses how Amazon has navigated past economic challenges by balancing cost management with long-term investments in strategic initiatives. For instance, during the 2008-2009 recession, Amazon continued to invest heavily in AWS, which has since grown into an $85 billion annual revenue business. Despite facing short-term headwinds in 2022, AWS remains focused on long-term customer relationships and helping customers optimize their cloud spend. AWS introduced new capabilities, including 3,300 new features and services in 2022, Graviton3 chips, and Trainium and Inferentia2 chips for machine learning.\n",
      "\n",
      "Moreover, the CEO mentions recent strategic shifts within the company, including the decision to discontinue certain businesses and initiatives that did not meet long-term potential, such as physical bookstores, 4-Star stores, Amazon Fabric, Amazon Care, and some newer devices. Additionally, Amazon amended certain programs, such as free shipping for all online grocery orders over $35, and made the difficult decision to eliminate 27,000 corporate roles. The company continues to evaluate its business landscape and adapt accordingly.\n",
      "\n",
      "The CEO remains optimistic about Amazon's future, noting the company's proactive approach to change and its ability to respond effectively to evolving market conditions. He emphasizes the importance of in-person collaboration for innovation and culture, and the decision for corporate employees to return to the office at least three days a week starting in May.\n",
      "\n",
      "Amazon's investment strategy focuses on opportunities that could be big, well-served, and differentiated, with the capability to achieve a reasonable return on invested capital. The company's early expansion from books to categories like Music, Video, Electronics, and Toys, and its international expansion, exemplify this approach. In 2022, Amazon's international consumer segment drove $118 billion in revenue, with established international markets showing remarkable growth rates. Amazon continues to invest in new international geographies, partnering with local entities to deliver solutions for customers and ultimately aiming to help more customers worldwide and build a larger free cash flow-generating consumer business.\n",
      "\n",
      "Beyond geographic expansion, Amazon has been working to expand its customer offerings across large, unique product retail market segments. The grocery market, valued at $800 billion in the US alone, represents a significant opportunity. Amazon has built a substantial grocery business over nearly 20 years, offering over three million items and focusing on larger pack sizes. To serve more of its customers' grocery needs, Amazon is exploring a broader physical store footprint and is experimenting with Amazon Fresh as a potential mass grocery format.\n",
      "\n",
      "Amazon Business is another significant investment, allowing businesses, municipalities, and organizations to procure products easily and at great savings. Launched in 2015, Amazon Business drives roughly $35 billion in annualized gross sales and serves over six million active customers, including 96 of the global Fortune 100 companies. The platform offers one-stop shopping, real-time analytics, and a broad selection of business supplies, with plans to continue building features based on customer feedback.\n",
      "\n",
      "Expanding internationally, pursuing large retail market segments that are still nascent for Amazon, and using unique assets to help merchants sell more effectively on their own websites are natural extensions for the company. Amazon has also ventured into new areas with unique opportunities. For example, Buy with Prime allows third-party brands and sellers to offer their products on their own websites to Amazon Prime members, providing them with fast, free Prime shipping and seamless checkout with their Amazon account. This initiative has increased shopper conversion on third-party shopping sites by 25% on average.\n",
      "\n",
      "Amazon's initial foray into healthcare began with pharmacy, which felt less like a major departure from ecommerce. Launched in 2020, Amazon Pharmacy offers transparent pricing, easy refills, and savings for Prime members. However, customers have expressed a desire for a broader healthcare experience. Amazon decided to start with primary care and acquired One Medical in July 2022. One Medical's patient-focused experience and digital app, along with its relationships with specialty physicians and local hospital systems, provide a strong foundation for Amazon's future healthcare business.\n",
      "\n",
      "Kuiper is another example of Amazon innovating for customers over the long term in an area where there’s high customer need. Amazon's vision for Kuiper is to create a low-Earth orbit satellite system to deliver high-quality broadband internet service to places around the world that don’t currently have it. The Kuiper system aims to provide reliable connectivity to hundreds of millions of households and businesses, enabling them to take online education courses, use financial services, start their own businesses, do their shopping, enjoy entertainment, and more. Kuiper will deliver not only accessibility, but affordability. Amazon's teams have developed low-cost antennas (customer terminals) that will lower the barriers to access. The new terminals are small, lightweight, and deliver speeds up to 400 megabits per second, powered by Amazon-designed baseband chips. Amazon plans to launch two prototype satellites to test the entire end-to-end communications network this year and be in beta with commercial customers in 2024. The customer reaction to what has been shared thus far about Kuiper has been very positive, and Amazon believes Kuiper represents a very large potential opportunity.\n",
      "\n",
      "Additionally, Amazon is heavily investing in Large Language Models (LLMs) and Generative AI. Machine learning has been a technology with high promise for several decades, and Generative AI, based on very Large Language Models, promises to significantly accelerate machine learning adoption. Amazon has been using machine learning extensively for 25 years and is now working on its own LLMs, which it believes will transform and improve virtually every customer experience. AWS is also democratizing this technology, offering the most price-performant machine learning chips in Trainium and Inferentia so companies of all sizes can afford to train and run their LLMs in production. Amazon is delivering applications like AWS’s CodeWhisperer, which revolutionizes developer productivity by generating code suggestions in real time.\n",
      "\n",
      "The CEO remains optimistic about Amazon's future, emphasizing the company's commitment to innovation, customer satisfaction, and strategic growth. He believes that as the company's consumer business and AWS continue to grow, and the global retail and IT markets shift towards digital and cloud solutions, Amazon will emerge from the current macroeconomic challenges in a stronger position. The CEO strongly believes that Amazon's best days are in front of it, and he looks forward to working with his teammates to make it so.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your document\n",
    "    document_path = \"../letters/2022-letter.txt\"\n",
    "    \n",
    "    # Summarize with different options\n",
    "    # Option 1: Standard refine chain (sequential processing, good for resource optimization)\n",
    "    summary = summarize_document(document_path, chunk_size=4000, chain_type=\"refine\")\n",
    "    \n",
    "    # Option 2: For comparison, you could try map_reduce (but be careful with service quotas)\n",
    "    # summary = summarize_document(document_path, chunk_size=4000, chain_type=\"map_reduce\")\n",
    "    \n",
    "    # Print the final summary\n",
    "    print(\"\\n=== FINAL SUMMARY ===\\n\")\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21988a-db39-4511-a426-b21feb75d2f2",
   "metadata": {},
   "source": [
    "You have now experimented with using prompt chunking and chaining with the LangChain framework to summarize large documents while mitigating issues arising from long input text.\n",
    "\n",
    "## Understanding the Key Components\n",
    "\n",
    "Let's review the key components of our solution:\n",
    "\n",
    "1. **Resource Optimization**: The `ResourceOptimizedLLM` wrapper manages API calls to stay within Bedrock service quotas by:\n",
    "   - Adding pauses between requests (controlled by `initial_pause`)\n",
    "   - Implementing exponential backoff with jitter when throttling occurs\n",
    "   - Providing comprehensive error handling and recovery\n",
    "\n",
    "2. **Document Chunking**: The `chunk_document` function splits large documents into manageable pieces:\n",
    "   - `chunk_size` controls the maximum size of each chunk (4000 characters)\n",
    "   - `chunk_overlap` ensures context continuity between chunks (200 characters)\n",
    "   - Natural text separators (`\\n\\n`, `\\n`, `.`, etc.) are used to avoid breaking mid-paragraph\n",
    "\n",
    "3. **Summarization Approaches**:\n",
    "   - **Refine Chain**: Processes chunks sequentially, refining the summary with each new chunk\n",
    "   - **Map-Reduce**: Summarizes each chunk independently, then combines and summarizes those summaries\n",
    "\n",
    "4. **Error Handling**: Comprehensive error handling ensures the process can recover from:\n",
    "   - Service throttling and capacity limits\n",
    "   - Access permission issues\n",
    "   - Other API errors\n",
    "\n",
    "## Try it yourself\n",
    "\n",
    "- Change the prompts to your specific usecase and evaluate the output of different models.\n",
    "- Experiment with different chunk sizes to find the optimal balance between context preservation and processing efficiency.\n",
    "- Try different summarization chain types (`refine` vs `map_reduce`) and compare the results.\n",
    "- Adjust the resource optimization parameters based on your Bedrock quota limits.\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "This approach can be applied to summarize various types of long-form content:\n",
    "- Customer service call transcripts\n",
    "- Meeting transcripts and notes\n",
    "- Research papers and technical documents\n",
    "- Legal documents and contracts\n",
    "- Books, articles, and blog posts\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "When implementing this solution in production:\n",
    "\n",
    "1. **Monitor API usage**: Keep track of your API calls to stay within quota limits\n",
    "2. **Optimize chunk size**: Balance between context preservation and processing efficiency\n",
    "3. **Implement proper error handling**: Ensure your application can gracefully handle API errors\n",
    "4. **Consider caching**: Cache results to avoid redundant API calls for frequently accessed documents\n",
    "5. **Test with various document types**: Different content may require different chunking strategies\n",
    "\n",
    "### Cleanup\n",
    "\n",
    "You have completed this notebook. To move to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file and continue with **Task 3**."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
