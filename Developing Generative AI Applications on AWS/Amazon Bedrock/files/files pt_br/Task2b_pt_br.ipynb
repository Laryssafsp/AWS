{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fded102b",
   "metadata": {},
   "source": [
    "# Tarefa 2b: Resumo de textos abstrativo\n",
    "\n",
    "Neste caderno, você gerencia os desafios decorrentes do resumo de documentos grandes. O texto de entrada pode exceder o tamanho do contexto do modelo, gerar saídas alucinadas ou acionar erros de falta de memória.\n",
    "\n",
    "Para mitigar esses problemas, esse caderno demonstra uma arquitetura que usa fragmentação e encadeamento de prompts com o framework [LangChain](https://python.langchain.com/docs/get_started/introduction.html), um toolkit que permite que aplicações usem modelos de linguagem.\n",
    "\n",
    "Você verá uma abordagem de cenários em que os documentos do usuário ultrapassam os limites de tokens. A fragmentação divide os documentos em segmentos menores do que os limites de comprimento do contexto antes de alimentá-los sequencialmente nos modelos. Isso encadeia prompts em blocos, mantendo o contexto anterior. Você aplica essa abordagem para resumir transcrições de chamadas, transcrições de reuniões, livros, artigos, publicações de blogs e outros conteúdos relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c1eaf9",
   "metadata": {},
   "source": [
    "## Tarefa 2b.1: Configurar o ambiente\n",
    "\n",
    "Nessa tarefa, você configura seu ambiente e cria um cliente Bedrock que detecta automaticamente sua região da AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f9067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a service client by name using the default session.\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "# AWS and Bedrock imports\n",
    "import boto3\n",
    "\n",
    "# Get the region programmatically\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name or \"us-east-1\"  # Default to us-east-1 if region not set\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name=region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae9a41",
   "metadata": {},
   "source": [
    "## Tarefa 2b.2: Resumir texto longo \n",
    "\n",
    "### Configurar o LangChain com o Boto3\n",
    "\n",
    "Nessa tarefa, você especifica o LLM para a classe LangChain Bedrock e passa argumentos para inferência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df2442",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LangChain imports\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "# Base LLM configuration\n",
    "modelId = \"amazon.nova-lite-v1:0\"\n",
    "\n",
    "class NovaLiteWrapper(LLM):\n",
    "    \"\"\"Wrapper for Nova Lite model that formats inputs correctly.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"nova-lite-wrapper\"\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Format prompt for Nova Lite and process.\"\"\"\n",
    "        # Format the prompt for Nova Lite's expected message structure\n",
    "        formatted_input = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"text\": prompt}]  # Content must be an array with text objects\n",
    "                }\n",
    "            ],\n",
    "            \"inferenceConfig\": {\n",
    "                \"maxTokens\": 2048,\n",
    "                \"temperature\": 0,\n",
    "                \"topP\": 0.9\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Call Bedrock directly with the properly formatted input\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=modelId,\n",
    "            body=json.dumps(formatted_input)\n",
    "        )\n",
    "        \n",
    "        # Parse the response - updated to handle Nova Lite's response format\n",
    "        response_body = json.loads(response['body'].read().decode('utf-8'))\n",
    "        \n",
    "        # Extract the text from the response\n",
    "        if 'output' in response_body and 'message' in response_body['output']:\n",
    "            message = response_body['output']['message']\n",
    "            if 'content' in message and isinstance(message['content'], list):\n",
    "                # Extract text from each content item\n",
    "                texts = []\n",
    "                for content_item in message['content']:\n",
    "                    if isinstance(content_item, dict) and 'text' in content_item:\n",
    "                        texts.append(content_item['text'])\n",
    "                return ' '.join(texts)\n",
    "        \n",
    "        # Fallback if the response format is different\n",
    "        return str(response_body)\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"model_id\": modelId}\n",
    "    \n",
    "    def get_num_tokens(self, text: str) -> int:\n",
    "        \"\"\"Estimate token count - Nova Lite uses roughly 1 token per 4 characters.\"\"\"\n",
    "        return len(text) // 4  # Rough approximation\n",
    "\n",
    "# Create the Nova Lite wrapper\n",
    "llm = NovaLiteWrapper()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d423aa-cb67-4170-afdb-b037f2531921",
   "metadata": {},
   "source": [
    "## Criação de wrapper de LLM otimizado para recursos\n",
    "\n",
    "Para lidar com as cotas de serviço do Bedrock de forma eficaz, criaremos uma classe wrapper que otimiza o uso de recursos e implementa um recuo exponencial com jitter para chamadas de API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a87119b-bf9d-4bec-be54-1efbcc9e8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced resource-optimized LLM wrapper with exponential backoff\n",
    "class ResourceOptimizedLLM(LLM):\n",
    "    \"\"\"Wrapper that optimizes resource usage for LLM processing.\"\"\"\n",
    "    \n",
    "    llm: Any  # The base LLM to wrap\n",
    "    min_pause: float = 30.0  # Minimum pause between requests\n",
    "    max_pause: float = 60.0  # Maximum pause after throttling\n",
    "    initial_pause: float = 10.0  # Initial pause between requests\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return f\"optimized-{self.llm._llm_type}\"\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Process with resource optimization and exponential backoff.\"\"\"\n",
    "        # Always pause between requests to optimize resource usage\n",
    "        time.sleep(self.initial_pause)\n",
    "        \n",
    "        # Implement retry with exponential backoff\n",
    "        max_retries = 10  # More retries for important operations\n",
    "        base_delay = self.min_pause\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"Making API call (attempt {attempt+1}/{max_retries})...\")\n",
    "                return self.llm._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                \n",
    "                # Handle different types of service exceptions\n",
    "                if any(err in error_str for err in [\"ThrottlingException\", \"TooManyRequests\", \"Rate exceeded\"]):\n",
    "                    if attempt < max_retries - 1:\n",
    "                        # Calculate backoff with jitter to prevent request clustering\n",
    "                        jitter = random.random() * 0.5\n",
    "                        wait_time = min(base_delay * (2 ** attempt) + jitter, self.max_pause)\n",
    "                        \n",
    "                        print(f\"Service capacity reached. Backing off for {wait_time:.2f} seconds...\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        print(\"Maximum retries reached. Consider reducing batch size or increasing delays.\")\n",
    "                        raise\n",
    "                else:\n",
    "                    # For non-capacity errors, don't retry\n",
    "                    print(f\"Non-capacity error: {error_str}\")\n",
    "                    raise\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {**self.llm._identifying_params, \"initial_pause\": self.initial_pause}\n",
    "    \n",
    "    def get_num_tokens(self, text: str) -> int:\n",
    "        \"\"\"Pass through token counting to the base model.\"\"\"\n",
    "        return self.llm.get_num_tokens(text)\n",
    "\n",
    "# Create the resource-optimized LLM\n",
    "resource_optimized_llm = ResourceOptimizedLLM(llm=llm, initial_pause=10.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b8886-b2e9-4239-8556-1f1483e9bfef",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Nota:** este wrapper adiciona recursos importantes para uso em produção:\n",
    "\n",
    "- Pausa automática entre solicitações para respeitar as cotas de serviço\n",
    "- Recuo exponencial com jitter para lidar com exceções de controle de utilização\n",
    "- Tratamento e geração de relatórios abrangentes de erros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31223056",
   "metadata": {},
   "source": [
    "## Tarefa 2b.3: Carregar arquivo de texto com muitos tokens\n",
    "\n",
    "Nessa tarefa, você usa uma cópia da [carta do CEO da Amazon aos acionistas em 2022](https://www.aboutamazon.com/news/company-news/amazon-ceo-andy-jassy-2022-letter-to-shareholders) no diretório de cartas. Você cria uma função para carregar o arquivo de texto e lidar com possíveis erros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70352ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Document loading function\n",
    "def load_document(file_path):\n",
    "    \"\"\"Load document from file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading document: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "shareholder_letter = \"../letters/2022-letter.txt\"\n",
    "letter = load_document(shareholder_letter)\n",
    "\n",
    "if letter:\n",
    "    num_tokens = resource_optimized_llm.get_num_tokens(letter)\n",
    "    print(f\"Document loaded successfully with {num_tokens} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0e622",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Nota:** você pode ignorar os avisos com segurança e prosseguir para a próxima célula. Resolveremos isso fragmentando o documento na próxima etapa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ec39d",
   "metadata": {},
   "source": [
    "## Tarefa 2b.4: Dividir o texto longo em blocos\n",
    "\n",
    "Nessa tarefa, você divide o texto em partes menores porque ele é muito longo para caber no prompt. `RecursiveCharacterTextSplitter` no LangChain suporta a divisão de texto longo em partes recursivamente até que o tamanho de cada bloco se torne menor que chunk_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c372b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Document chunking with conservative settings\n",
    "def chunk_document(text, chunk_size=4000, chunk_overlap=200):\n",
    "    \"\"\"Split document into manageable chunks.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.create_documents([text])\n",
    "    print(f\"Document split into {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "# Split the document into chunks\n",
    "if letter:\n",
    "    docs = chunk_document(letter, chunk_size=4000, chunk_overlap=200)\n",
    "    \n",
    "    if docs:\n",
    "        num_docs = len(docs)\n",
    "        num_tokens_first_doc = resource_optimized_llm.get_num_tokens(docs[0].page_content)\n",
    "        print(f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acedb37-52f7-40ac-ae70-cede47b270a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Nota:** o parâmetro `chunk_size` controla o tamanho de cada bloco. Blocos maiores fornecem mais contexto, mas exigem mais recursos de processamento. O parâmetro `chunk_overlap` garante alguma continuidade entre os blocos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f8ae45",
   "metadata": {},
   "source": [
    "## Tarefa 2b.5: Resumir e reunir blocos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d49f5",
   "metadata": {},
   "source": [
    "Nessa tarefa, você implementa duas abordagens para resumir documentos fragmentados: usando a cadeia de resumo integrada do LangChain e uma implementação manual personalizada que fornece melhor controle sobre o uso de recursos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc48123-d9f4-4275-a509-4ed0a687d547",
   "metadata": {},
   "source": [
    "## Entender abordagens de implementação\n",
    "\n",
    "Este caderno demonstra duas abordagens diferentes para resumir documentos grandes com o AWS Bedrock:\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Nota:** incluímos uma implementação padrão do LangChain e uma implementação personalizada para mostrar as vantagens e desvantagens entre conveniência e controle ao criar aplicativos de produção.\n",
    "\n",
    "### Dois caminhos para o mesmo objetivo\n",
    "\n",
    "1. **Implementação padrão do LangChain** (`process_documents_with_pacing`):\n",
    "   - Usa as cadeias de resumo integradas do LangChain\n",
    "   - Requer menos código e é mais fácil de implementar\n",
    "   - Abstrai a complexidade subjacente\n",
    "   - Ótimo para prototipagem rápida e casos de uso simples\n",
    "\n",
    "2. **Implementação de refinamento personalizada** (`manual_refine_with_optimization`):\n",
    "   - Constrói o processo de refinamento passo a passo\n",
    "   - Fornece visibilidade completa dos prompts e do processamento\n",
    "   - Oferece tratamento granular de erros para cada bloco do documento\n",
    "   - Permite um controle preciso sobre o tempo de chamada da API e a lógica de repetição\n",
    "\n",
    "Embora ambas alcancem o mesmo resultado final, a implementação personalizada oferece mais controle sobre todo o processo, o que é crucial ao trabalhar com cotas de serviço e criar aplicativos prontos para produção.\n",
    "\n",
    "Em cenários reais, você pode começar com a implementação padrão durante o desenvolvimento e depois passar para uma implementação personalizada quando precisar de mais controle sobre o uso de recursos, tratamento de erros ou engenharia de prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a2fa5-a989-4330-ad46-72ddc631b7b8",
   "metadata": {},
   "source": [
    "### Implementação padrão do LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56f7535-31cf-4809-ba1b-a7ef5e3196a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom document processing with controlled pacing\n",
    "def process_documents_with_pacing(docs, chain_type=\"refine\", verbose=True):\n",
    "    \"\"\"Process documents with pacing to optimize resource usage.\"\"\"\n",
    "    \n",
    "    # Configure the chain\n",
    "    summary_chain = load_summarize_chain(\n",
    "        llm=resource_optimized_llm,\n",
    "        chain_type=chain_type,  # \"refine\" processes sequentially, good for resource optimization\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Process with additional error handling\n",
    "    try:\n",
    "        result = summary_chain.invoke(docs)\n",
    "        return result\n",
    "    except ValueError as error:\n",
    "        if \"AccessDeniedException\" in str(error):\n",
    "            print(f\"\\n\\033[91mAccess Denied: {error}\\033[0m\")\n",
    "            print(\"\\nTo troubleshoot this issue, please check:\")\n",
    "            print(\"1. Your IAM permissions for Bedrock\")\n",
    "            print(\"2. Model access permissions\")\n",
    "            print(\"3. AWS credentials configuration\")\n",
    "            return {\"output_text\": \"Error: Access denied. Check permissions.\"}\n",
    "        else:\n",
    "            print(f\"\\n\\033[91mError during processing: {error}\\033[0m\")\n",
    "            return {\"output_text\": f\"Error during processing: {str(error)}\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988cc73-0982-4b69-88cb-7066176d29b5",
   "metadata": {},
   "source": [
    "### Implementação personalizada de refinamento com otimização aprimorada de recursos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216651f-0fc4-42e6-93c8-8adf65b729a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual implementation of resource-optimized processing for refine chain\n",
    "def manual_refine_with_optimization(docs, llm, verbose=True):\n",
    "    \"\"\"Manually implement refine chain with resource optimization.\"\"\"\n",
    "    if not docs:\n",
    "        return {\"output_text\": \"No documents to process.\"}\n",
    "    \n",
    "    # Process first document to get initial summary\n",
    "    print(f\"Processing initial document (1/{len(docs)})...\")\n",
    "    \n",
    "    # Simple prompt for initial document\n",
    "    initial_prompt = \"\"\"Write a concise summary of the following:\n",
    "    \"{text}\"\n",
    "    CONCISE SUMMARY:\"\"\"\n",
    "    \n",
    "    # Process first document\n",
    "    try:\n",
    "        current_summary = llm(initial_prompt.format(text=docs[0].page_content))\n",
    "        print(\"Initial summary created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating initial summary: {e}\")\n",
    "        return {\"output_text\": \"Failed to create initial summary.\"}\n",
    "    \n",
    "    # Process remaining documents with refine approach\n",
    "    for i, doc in enumerate(docs[1:], start=2):\n",
    "        print(f\"Refining with document {i}/{len(docs)}...\")\n",
    "        \n",
    "        # Refine prompt\n",
    "        refine_prompt = \"\"\"Your job is to refine an existing summary.\n",
    "        We have an existing summary: {existing_summary}\n",
    "        \n",
    "        We have a new document to add information from: {text}\n",
    "        \n",
    "        Please update the summary to incorporate new information from the document.\n",
    "        If the document doesn't contain relevant information, return the existing summary.\n",
    "        \n",
    "        REFINED SUMMARY:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Apply resource optimization between requests\n",
    "            time.sleep(10.0)  # Base delay between requests\n",
    "            \n",
    "            # Update the summary\n",
    "            current_summary = llm(refine_prompt.format(\n",
    "                existing_summary=current_summary,\n",
    "                text=doc.page_content\n",
    "            ))\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Successfully refined with document {i}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during refinement with document {i}: {e}\")\n",
    "            # Apply exponential backoff\n",
    "            backoff = min(10.0 * (2 ** (i % 5)) + (random.random() * 2), 30)\n",
    "            print(f\"Backing off for {backoff:.2f} seconds...\")\n",
    "            time.sleep(backoff)\n",
    "            \n",
    "            # Try one more time\n",
    "            try:\n",
    "                current_summary = llm(refine_prompt.format(\n",
    "                    existing_summary=current_summary,\n",
    "                    text=doc.page_content\n",
    "                ))\n",
    "            except Exception as retry_error:\n",
    "                print(f\"Retry failed for document {i}: {retry_error}\")\n",
    "                # Continue with current summary rather than failing completely\n",
    "    \n",
    "    return {\"output_text\": current_summary}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d2892-54d2-43a1-a69c-58a45432cfe1",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Nota:** a implementação manual oferece mais controle sobre:\n",
    "\n",
    "- Os prompts exatos usados para resumir\n",
    "- Tratamento de erros e recuperação\n",
    "- Otimização de recursos entre chamadas de API\n",
    "- Degradação graciosa quando ocorrem erros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c914f0-cbc3-48b4-a9ee-f14d02ce921e",
   "metadata": {},
   "source": [
    "## Tarefa 2b.6: Função de execução principal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e4d11-a876-4bd9-9d91-404017452916",
   "metadata": {},
   "source": [
    "Agora, criaremos uma função principal que orquestra todo o processo de resumo do documento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819efba-6a3f-4aaa-8668-13b45993af10",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Nota:** a função principal (`summarize_document`) permite que você escolha qual implementação usar com base no parâmetro `chain_type`, facilitando a comparação de resultados e desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d0718-2adf-4bf9-bf18-acf4644115d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution function\n",
    "def summarize_document(file_path, chunk_size=4000, chain_type=\"refine\"):\n",
    "    \"\"\"Main function to summarize a document.\"\"\"\n",
    "    \n",
    "    print(f\"Starting document summarization process for: {file_path}\")\n",
    "    \n",
    "    # Load the document\n",
    "    document_text = load_document(file_path)\n",
    "    if not document_text:\n",
    "        return \"Failed to load document.\"\n",
    "    \n",
    "    print(f\"Document loaded successfully. Length: {len(document_text)} characters\")\n",
    "    \n",
    "    # Split into chunks\n",
    "    docs = chunk_document(document_text, chunk_size=chunk_size, chunk_overlap=200)\n",
    "    \n",
    "    # If document is very large, provide a warning\n",
    "    if len(docs) > 15:\n",
    "        print(f\"Warning: Document is large ({len(docs)} chunks). Processing may take some time.\")\n",
    "        \n",
    "        # For very large documents, consider using a subset for testing\n",
    "        if len(docs) > 30:\n",
    "            print(\"Document is extremely large. Consider using a smaller chunk_size or processing a subset.\")\n",
    "            # Optional: process only a subset for testing\n",
    "            # docs = docs[:15]\n",
    "    \n",
    "    # Process the documents\n",
    "    print(f\"Processing document using '{chain_type}' chain type...\")\n",
    "    \n",
    "    # Use the appropriate processing method based on chain type\n",
    "    if chain_type == \"refine\":\n",
    "        # Use our manual implementation for better control over resource optimization\n",
    "        result = manual_refine_with_optimization(docs, resource_optimized_llm)\n",
    "    else:\n",
    "        # Use standard LangChain implementation for other chain types\n",
    "        result = process_documents_with_pacing(docs, chain_type=chain_type)\n",
    "    \n",
    "    # Return the result\n",
    "    if result and \"output_text\" in result:\n",
    "        print(\"\\nSummarization completed successfully!\")\n",
    "        return result[\"output_text\"]\n",
    "    else:\n",
    "        print(\"\\nSummarization failed or returned no result.\")\n",
    "        return \"Summarization process did not produce a valid result.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcfca15-b18e-4aba-8a09-2500fc941f8e",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i>**Nota: ** Dependendo do número de documentos, da cota da taxa de solicitações do Bedrock e das configurações de repetição definidas, o processo de resumo pode levar algum tempo para ser executado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a31741-103d-4524-b3a8-3c68da2baa07",
   "metadata": {},
   "source": [
    "## Tarefa 2b.7: Executar o resumo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0de4dd-9430-46fe-ae9f-8af121f41b51",
   "metadata": {},
   "source": [
    "Vamos fazer um resumo da carta para os acionistas. Por padrão, a função summarize_document() usa a cadeia de refinamento. Para ativar o map_reduce: \n",
    "\n",
    "- Incluir comentário na seguinte linha: `summary = summarize_document(document_path, chunk_size=4000, chain_type=\"refine\")`\n",
    "- Remover o comentário da seguinte linha: `# summary = summarize_document(document_path, chunk_size=4000, chain_type=\"map_reduce\")`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff42a88c-44af-4b35-8eed-d8ecd3ea38df",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Nota:** não se preocupe se você notar mensagens de erro durante a execução. Seu código inclui um tratamento robusto de erros que repetirá automaticamente as solicitações que falharem com recuo exponencial. Esse é um comportamento normal ao trabalhar com cotas de serviço e demonstra como os aplicativos prontos para produção devem lidar com as limitações da API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ffd26-bfbd-47d1-b6d7-76daabc8e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your document\n",
    "    document_path = \"../letters/2022-letter.txt\"\n",
    "    \n",
    "    # Summarize with different options\n",
    "    # Option 1: Standard refine chain (sequential processing, good for resource optimization)\n",
    "    summary = summarize_document(document_path, chunk_size=4000, chain_type=\"refine\")\n",
    "    \n",
    "    # Option 2: For comparison, you could try map_reduce (but be careful with service quotas)\n",
    "    # summary = summarize_document(document_path, chunk_size=4000, chain_type=\"map_reduce\")\n",
    "    \n",
    "    # Print the final summary\n",
    "    print(\"\\n=== FINAL SUMMARY ===\\n\")\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21988a-db39-4511-a426-b21feb75d2f2",
   "metadata": {},
   "source": [
    "Você testou o uso da fragmentação e do encadeamento de prompts com o framework LangChain para resumir documentos grandes e, ao mesmo tempo, mitigar problemas decorrentes de textos longos de entrada.\n",
    "\n",
    "## Para entender os principais componentes\n",
    "\n",
    "Vamos analisar os principais componentes da nossa solução:\n",
    "\n",
    "1. **Otimização de recursos**: o wrapper `ResourceOptimizedLLM` gerencia as chamadas de API para permanecer dentro das cotas de serviço do Bedrock:\n",
    "   - Adicionando pausas entre solicitações (controlado por `initial_pause`)\n",
    "   - Implementando recuo exponencial com jitter quando ocorre controle de utilização\n",
    "   - Fornecendo tratamento e recuperação abrangentes de erros\n",
    "\n",
    "2. **Fragmentação de documentos**: a função `chunk_document` divide documentos grandes em partes gerenciáveis:\n",
    "   - `chunk_size` controla o tamanho máximo de cada bloco (4.000 caracteres)\n",
    "   - `chunk_overlap` garante a continuidade do contexto entre os blocos (200 caracteres)\n",
    "   - Separadores de texto natural (`\\n\\n`, `\\n`, `.` etc.) são usados para evitar quebrar o meio do parágrafo\n",
    "\n",
    "3. **Abordagens de resumo**:\n",
    "   - **Refinar a cadeia**: processa os blocos sequencialmente, refinando o resumo com cada novo bloco\n",
    "   - **Map-Reduce**: resume cada bloco de forma independente e, em seguida, reúne e resume esses resumos\n",
    "\n",
    "4. **Tratamento de erros**: o tratamento abrangente de erros garante que o processo possa se recuperar de:\n",
    "   - Controle de utilização do serviço e limitação da capacidade\n",
    "   - Problemas de permissão de acesso\n",
    "   - Outros erros de API\n",
    "\n",
    "## Experimente você mesmo\n",
    "\n",
    "- Altere os prompts para seu caso de uso específico e avalie o resultado de diferentes modelos.\n",
    "- Experimente diferentes tamanhos de blocos para encontrar o equilíbrio ideal entre preservação de contexto e eficiência de processamento.\n",
    "- Experimente diferentes tipos de cadeia de resumo (`refine` versus `map_reduce`) e compare os resultados.\n",
    "- Ajuste os parâmetros de otimização de recursos com base nos limites de cota do Bedrock.\n",
    "\n",
    "### Aplicações práticas\n",
    "\n",
    "Essa abordagem pode ser aplicada para resumir vários tipos de conteúdo longo:\n",
    "- Transcrições de chamadas de atendimento ao cliente\n",
    "- Transcrições e notas de reuniões\n",
    "- Artigos de pesquisa e documentos técnicos\n",
    "- Documentos jurídicos e contratos\n",
    "- Livros, artigos e postagens em blogs\n",
    "\n",
    "### Práticas recomendadas\n",
    "\n",
    "Ao implementar essa solução na produção:\n",
    "\n",
    "1. **Monitore o uso da API**: acompanhe suas chamadas de API para ficar dentro dos limites da cota\n",
    "2. **Otimize tamanho do bloco**: equilíbrio entre preservação de contexto e eficiência de processamento\n",
    "3. **Implemente o tratamento adequado de erros**: garanta que seu aplicativo possa lidar corretamente com os erros da API\n",
    "4. **Considere o armazenamento em cache**: armazene os resultados em cache para evitar chamadas de API redundantes para documentos acessados com frequência\n",
    "5. **Teste vários tipos de documentos**: conteúdos diferentes podem exigir diferentes estratégias de fragmentação\n",
    "\n",
    "### Limpeza\n",
    "\n",
    "Você concluiu este caderno. Passe para a próxima parte do laboratório da seguinte forma:\n",
    "\n",
    "- Feche este arquivo de caderno e continue com a **Tarefa 3**."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}