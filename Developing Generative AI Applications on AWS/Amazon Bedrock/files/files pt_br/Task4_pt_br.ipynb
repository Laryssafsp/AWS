{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarefa 4: interface conversacional, chat os LLMs Nova Lite e Titan Embeddings\n",
    "\n",
    "Neste caderno, você desenvolve um chatbot utilizando os modelos Foundation Nova Lite e Titan Embeddings (FMs) disponíveis no Amazon Bedrock.\n",
    "\n",
    "Interfaces conversacionais, como chatbots e assistentes virtuais, podem aprimorar a experiência do usuário para seus clientes. Os chatbots usam algoritmos de processamento de linguagem natural (PLN) e machine learning para entender e responder às consultas dos usuários. Você pode usar chatbots em diversas aplicações, como atendimento ao cliente, vendas e comércio eletrônico, para oferecer respostas rápidas e eficientes aos usuários. Os usuários podem acessá-los por vários canais, como sites, plataformas de redes sociais e aplicativos de mensagens.\n",
    "\n",
    "- **Chatbot (básico)**, chatbot zero-shot com modelo FM\n",
    "- **Chatbot com uso de prompt**, template(LangChain): chatbot com algum contexto fornecido no modelo de prompt\n",
    "- **Chatbot com persona**: chatbot com funções definidas, por exemplo, coach de carreira e interações humanas\n",
    "- **Chatbot com consciência contextual**, inclusão de contexto por meio de um arquivo externo gerando incorporações.\n",
    "\n",
    "## Framework LangChain para criação de chatbot com o Amazon Bedrock\n",
    "\n",
    "Em interfaces conversacionais, como chatbots, lembrar as interações anteriores é muito importante, tanto no curto quanto no longo prazo.\n",
    "\n",
    "O framework LangChain fornece componentes de memória em duas formas. Primeiro, o LangChain oferece recursos auxiliares para gerenciar e manipular mensagens de chat anteriores. Eles são projetados para serem modulares. Em segundo lugar, o LangChain fornece maneiras fáceis de incorporar esses utilitários em cadeias, permitindo que você defina e interaja facilmente com diferentes tipos de abstrações, o que facilita a criação de chatbots avançados.\n",
    "\n",
    "## Criar chatbot com contexto: elementos principais\n",
    "\n",
    "O primeiro processo na criação de um chatbot com consciência contextual é gerar incorporações para o contexto. Normalmente, ocorre um processo de ingestão no seu modelo para gerar as incorporações, que depois são inseridas no armazenamento de vetores. Neste caderno, você usa o modelo do Titan Embeddings para isso. O segundo processo é a orquestração, interação, invocação e o retorno dos resultados das solicitações dos usuários. Isso envolve orquestrar a solicitação do usuário, interagir com os modelos/componentes necessários para coletar informações, invocar o chatbot para formular uma resposta e, em seguida, retornar a resposta do chatbot ao usuário."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 4.1: Configurar o ambiente\n",
    "\n",
    "Nesta tarefa, você configurará o ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:39:49.572480Z",
     "iopub.status.busy": "2025-07-16T13:39:49.572128Z",
     "iopub.status.idle": "2025-07-16T13:39:49.586386Z",
     "shell.execute_reply": "2025-07-16T13:39:49.585565Z",
     "shell.execute_reply.started": "2025-07-16T13:39:49.572450Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ignore warnings and create a service client by name using the default session.\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import boto3\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "bedrock_client = boto3.client('bedrock-runtime',region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilitários de otimização de recursos\n",
    "\n",
    "Nesta seção, criamos um decorador que otimiza o uso de recursos da API e melhora a confiabilidade das solicitações. Esse decorador implementa várias práticas recomendadas para trabalhar com serviços de IA baseados em nuvem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:39:51.477822Z",
     "iopub.status.busy": "2025-07-16T13:39:51.477559Z",
     "iopub.status.idle": "2025-07-16T13:39:51.484891Z",
     "shell.execute_reply": "2025-07-16T13:39:51.483880Z",
     "shell.execute_reply.started": "2025-07-16T13:39:51.477803Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from functools import wraps\n",
    "\n",
    "# Resource optimization decorator for API efficiency\n",
    "def optimize_resource_usage(max_attempts=10, base_pause=10.0):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(max_attempts):\n",
    "                try:\n",
    "                    # Space out requests for optimal service performance\n",
    "                    if attempt > 0:\n",
    "                        time.sleep(10.0)\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    error_str = str(e)\n",
    "                    # Check if this is a service capacity exception\n",
    "                    if any(err in error_str for err in [\"ThrottlingException\", \"TooManyRequests\", \n",
    "                                                     \"Too many tokens\", \"Rate exceeded\"]):\n",
    "                        if attempt < max_attempts - 1:\n",
    "                            # Implement progressive backoff with jitter\n",
    "                            jitter = random.random() * 0.5\n",
    "                            wait_time = min(base_pause * (2 ** attempt) + jitter, 60)\n",
    "                            \n",
    "                            print(f\"⏱️ Optimizing request timing. Pausing for {wait_time:.1f}s (attempt {attempt+1}/{max_attempts})\")\n",
    "                            time.sleep(wait_time)\n",
    "                        else:\n",
    "                            print(\"⚠️ Maximum attempts reached. Consider spacing out operations.\")\n",
    "                            raise\n",
    "                    else:\n",
    "                        raise\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para entender a otimização de recursos\n",
    "\n",
    "O código acima implementa uma abordagem inteligente para o gerenciamento de APIs que:\n",
    "\n",
    "- **Espaça as solicitações** para manter a produtividade ideal\n",
    "- **Tenta de novo automaticamente** quando os limites de capacidade temporários são atingidos\n",
    "- **Implementa o recuo exponencial** aumentando progressivamente os tempos de espera\n",
    "- **Adiciona jitter aleatório** para evitar o agrupamento de solicitações\n",
    "\n",
    "Essas técnicas são práticas padrão do setor para trabalhar com serviços de IA baseados em nuvem e garantem que seu aplicativo permaneça responsivo mesmo sob condições desafiadoras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes aprimoradas de modelos Bedrock\n",
    "\n",
    "Em seguida, criamos versões especializadas das classes do modelo Bedrock que incorporam automaticamente as técnicas de otimização de recursos. Essas classes estendem as implementações padrão do LangChain e, ao mesmo tempo, adicionam recursos de confiabilidade que são essenciais para aplicativos de produção.\n",
    "\n",
    "Ao agrupar os métodos críticos da API com nosso decorador de otimização, garantimos um comportamento consistente ao nos comunicarmos com os serviços Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:39:53.911030Z",
     "iopub.status.busy": "2025-07-16T13:39:53.910632Z",
     "iopub.status.idle": "2025-07-16T13:39:53.925725Z",
     "shell.execute_reply": "2025-07-16T13:39:53.925067Z",
     "shell.execute_reply.started": "2025-07-16T13:39:53.911008Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "\n",
    "# Enhanced ChatBedrock with resource optimization\n",
    "class ResourceOptimizedChatBedrock(ChatBedrock):\n",
    "    \"\"\"Wrapper that optimizes resource usage and handles service capacity constraints.\"\"\"\n",
    "    \n",
    "    @optimize_resource_usage(max_attempts=10)\n",
    "    def _call_model(self, *args, **kwargs):\n",
    "        return super()._call_model(*args, **kwargs)\n",
    "    \n",
    "    @optimize_resource_usage(max_attempts=10)\n",
    "    def _generate(self, *args, **kwargs):\n",
    "        return super()._generate(*args, **kwargs)\n",
    "        \n",
    "# Enhanced BedrockEmbeddings with resource optimization\n",
    "class ResourceOptimizedEmbeddings(BedrockEmbeddings):\n",
    "    \"\"\"Wrapper for BedrockEmbeddings with intelligent resource management.\"\"\"\n",
    "    \n",
    "    @optimize_resource_usage(max_attempts=10)\n",
    "    def _embed_documents(self, texts):\n",
    "        return super()._embed_documents(texts)\n",
    "        \n",
    "    @optimize_resource_usage(max_attempts=10)\n",
    "    def _embed_query(self, text):\n",
    "        return super()._embed_query(text)\n",
    "        \n",
    "    @optimize_resource_usage(max_attempts=10)\n",
    "    def embed_documents(self, texts):\n",
    "        return super().embed_documents(texts)\n",
    "        \n",
    "    @optimize_resource_usage(max_attempts=10)\n",
    "    def embed_query(self, text):\n",
    "        return super().embed_query(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Nota:** essas classes aprimoradas:\n",
    "\n",
    "- Aplicam a otimização de recursos a todos os métodos críticos da API\n",
    "- Mantêm total compatibilidade com as interfaces padrão do LangChain\n",
    "- Lidam automaticamente com problemas intermitentes de capacidade de serviço\n",
    "- Fornecem um comportamento consistente nas operações de bate-papo e incorporação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construir memória conversacional\n",
    "\n",
    "Uma das características mais importantes de um chatbot eficaz é a capacidade de lembrar interações anteriores. Sem memória, cada mensagem seria tratada isoladamente, criando uma experiência desarticulada.\n",
    "\n",
    "Nesta seção, implementamos a memória conversacional usando a classe `InMemoryChatMessageHistory` LangChain. Isso permite que nosso chatbot faça referência a trocas anteriores e mantenha o contexto durante toda a conversa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilitários de formatação de conversas\n",
    "\n",
    "Esta seção implementa uma função utilitária que formata corretamente as conversas multifuncionais para grandes modelos de linguagem. O formato está alinhado aos requisitos específicos do formato de token do modelo Nova Lite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:39:56.434827Z",
     "iopub.status.busy": "2025-07-16T13:39:56.434462Z",
     "iopub.status.idle": "2025-07-16T13:39:56.440244Z",
     "shell.execute_reply": "2025-07-16T13:39:56.439394Z",
     "shell.execute_reply.started": "2025-07-16T13:39:56.434787Z"
    }
   },
   "outputs": [],
   "source": [
    "# format instructions into a conversational prompt\n",
    "from typing import Dict, List\n",
    "\n",
    "def format_instructions(instructions: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format instructions where conversation roles must alternate system/user/assistant/user/assistant/...\"\"\"\n",
    "    prompt: List[str] = []\n",
    "    for instruction in instructions:\n",
    "        if instruction[\"role\"] == \"system\":\n",
    "            prompt.extend([\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\", (instruction[\"content\"]).strip(), \" <|eot_id|>\"])\n",
    "        elif instruction[\"role\"] == \"user\":\n",
    "            prompt.extend([\"<|start_header_id|>user<|end_header_id|>\\n\", (instruction[\"content\"]).strip(), \" <|eot_id|>\"])\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid role: {instruction['role']}. Role must be either 'user' or 'system'.\")\n",
    "    prompt.extend([\"<|start_header_id|>assistant<|end_header_id|>\\n\"])\n",
    "    return \"\".join(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tarefa 4.2: usar o histórico do chat do LangChain para iniciar a conversa\n",
    "\n",
    "Nesta tarefa, você ativará o chatbot para que guarde o contexto das conversas entre as várias interações com os usuários. Ter uma memória conversacional é imprescindível para que os chatbots mantenham diálogos significativos e coerentes ao longo do tempo.\n",
    "\n",
    "Você implementa recursos de memória conversacional com base na classe InMemoryChatMessageHistory do LangChain. Esse objeto armazena as conversas entre o usuário e o chatbot, e o histórico fica disponível para que o agente do chatbot possa aproveitar o contexto de uma conversa anterior.\n",
    "\n",
    "<i aria-hidden=\"true\" class=\"fas fa-sticky-note\" style=\"color:#563377\"></i> **Nota:** as saídas do modelo não são determinísticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:39:58.784574Z",
     "iopub.status.busy": "2025-07-16T13:39:58.784233Z",
     "iopub.status.idle": "2025-07-16T13:39:59.374998Z",
     "shell.execute_reply": "2025-07-16T13:39:59.374392Z",
     "shell.execute_reply.started": "2025-07-16T13:39:58.784553Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "chat_model = ResourceOptimizedChatBedrock(\n",
    "    model_id=\"amazon.nova-lite-v1:0\", \n",
    "    client=bedrock_client\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Answer the following questions as best you can.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory()\n",
    "\n",
    "\n",
    "def get_history():\n",
    "    return history\n",
    "\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_history,\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "query=\"how are you?\"\n",
    "response=wrapped_chain.invoke({\"input\": query})\n",
    "# Printing history to see the history being built out. \n",
    "print(history)\n",
    "# For the rest of the conversation, the output will only include response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novas perguntas\n",
    "\n",
    "O modelo respondeu com uma mensagem inicial. Agora, você faz algumas perguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:40:01.902338Z",
     "iopub.status.busy": "2025-07-16T13:40:01.901392Z",
     "iopub.status.idle": "2025-07-16T13:40:05.452053Z",
     "shell.execute_reply": "2025-07-16T13:40:05.451355Z",
     "shell.execute_reply.started": "2025-07-16T13:40:01.902306Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#new questions\n",
    "instructions = [{\"role\": \"user\", \"content\": \"Give me a few tips on how to start a new garden.\"}]\n",
    "response=wrapped_chain.invoke({\"input\": format_instructions(instructions)})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprimorar as perguntas\n",
    "\n",
    "Agora, faça uma pergunta sem mencionar a palavra jardim para ver se a modelo consegue entender a conversa anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:40:05.453412Z",
     "iopub.status.busy": "2025-07-16T13:40:05.453072Z",
     "iopub.status.idle": "2025-07-16T13:40:08.847915Z",
     "shell.execute_reply": "2025-07-16T13:40:08.847214Z",
     "shell.execute_reply.started": "2025-07-16T13:40:05.453391Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build on the questions\n",
    "instructions = [{\"role\": \"user\", \"content\": \"bugs\"}]\n",
    "response=wrapped_chain.invoke({\"input\": format_instructions(instructions)})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encerrar a conversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:40:08.849523Z",
     "iopub.status.busy": "2025-07-16T13:40:08.849025Z",
     "iopub.status.idle": "2025-07-16T13:40:09.271075Z",
     "shell.execute_reply": "2025-07-16T13:40:09.270496Z",
     "shell.execute_reply.started": "2025-07-16T13:40:08.849424Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# finishing the conversation\n",
    "instructions = [{\"role\": \"user\", \"content\": \"That's all, thank you!\"}]\n",
    "response=wrapped_chain.invoke({\"input\": format_instructions(instructions)})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 4.3: Chatbot usando modelo de prompt (LangChain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de modelos de prompt para interações consistentes\n",
    "\n",
    "A engenharia de prompts é um aspecto essencial do trabalho com grandes modelos de linguagem. Usando modelos, podemos estruturar nossos prompts de forma consistente e controlar como o modelo responde em diferentes cenários.\n",
    "\n",
    "Os modelos de prompt do LangChain fornecem uma maneira padronizada de formatar entradas para diferentes funções de conversa (sistema, usuário, assistente), garantindo que o modelo receba um contexto adequadamente estruturado.\n",
    "\n",
    "Nesta tarefa, você usa o PromptTemplate padrão que é responsável pela construção dessa entrada. O LangChain oferece várias classes e funções para facilitar a criação e trabalho com prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:40:13.625782Z",
     "iopub.status.busy": "2025-07-16T13:40:13.624923Z",
     "iopub.status.idle": "2025-07-16T13:40:13.630863Z",
     "shell.execute_reply": "2025-07-16T13:40:13.629737Z",
     "shell.execute_reply.started": "2025-07-16T13:40:13.625751Z"
    }
   },
   "outputs": [],
   "source": [
    "#  prompt for a conversational agent\n",
    "def format_prompt(actor:str, input:str):\n",
    "    formatted_prompt: List[str] = []\n",
    "    if actor == \"system\":\n",
    "        prompt_template=\"\"\"<|begin_of_text|><|start_header_id|>{actor}<|end_header_id|>\\n{input}<|eot_id|>\"\"\"\n",
    "    elif actor == \"user\":\n",
    "        prompt_template=\"\"\"<|start_header_id|>{actor}<|end_header_id|>\\n{input}<|eot_id|>\"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid role: {actor}. Role must be either 'user' or 'system'.\")   \n",
    "    prompt = PromptTemplate.from_template(prompt_template)     \n",
    "    formatted_prompt.extend(prompt.format(actor=actor,input=input))\n",
    "    formatted_prompt.extend([\"<|start_header_id|>assistant<|end_header_id|>\\n\"])\n",
    "    return \"\".join(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:40:15.802793Z",
     "iopub.status.busy": "2025-07-16T13:40:15.802409Z",
     "iopub.status.idle": "2025-07-16T13:40:15.961809Z",
     "shell.execute_reply": "2025-07-16T13:40:15.961062Z",
     "shell.execute_reply.started": "2025-07-16T13:40:15.802770Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chat user experience\n",
    "import ipywidgets as ipw\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class ChatUX:\n",
    "    \"\"\" A chat UX using IPWidgets\n",
    "    \"\"\"\n",
    "    def __init__(self, qa, retrievalChain = False):\n",
    "        self.qa = qa\n",
    "        self.name = None\n",
    "        self.b=None\n",
    "        self.retrievalChain = retrievalChain\n",
    "        self.out = ipw.Output()\n",
    "\n",
    "\n",
    "    def start_chat(self):\n",
    "        print(\"Starting chat bot\")\n",
    "        display(self.out)\n",
    "        self.chat(None)\n",
    "\n",
    "\n",
    "    def chat(self, _):\n",
    "        if self.name is None:\n",
    "            prompt = \"\"\n",
    "        else: \n",
    "            prompt = self.name.value\n",
    "        if 'q' == prompt or 'quit' == prompt or 'Q' == prompt:\n",
    "            with self.out:\n",
    "                print(\"Thank you , that was a nice chat !!\")\n",
    "            return\n",
    "        elif len(prompt) > 0:\n",
    "            with self.out:\n",
    "                thinking = ipw.Label(value=\"Thinking...\")\n",
    "                display(thinking)\n",
    "                try:\n",
    "                    if self.retrievalChain:\n",
    "                        response = self.qa.invoke({\"input\": prompt})\n",
    "                        result=response['answer']\n",
    "                    else:\n",
    "                        instructions = [{\"role\": \"user\", \"content\": prompt}]\n",
    "                        #result = self.qa.invoke({'input': format_prompt(\"user\",prompt)}) #, 'history':chat_history})\n",
    "                        result = self.qa.invoke({\"input\": format_instructions(instructions)})\n",
    "                except:\n",
    "                    result = \"No answer\"\n",
    "                thinking.value=\"\"\n",
    "                print(f\"AI:{result}\")\n",
    "                self.name.disabled = True\n",
    "                self.b.disabled = True\n",
    "                self.name = None\n",
    "\n",
    "        if self.name is None:\n",
    "            with self.out:\n",
    "                self.name = ipw.Text(description=\"You:\", placeholder='q to quit')\n",
    "                self.b = ipw.Button(description=\"Send\")\n",
    "                self.b.on_click(self.chat)\n",
    "                display(ipw.Box(children=(self.name, self.b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, inicie uma conversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:59:19.979425Z",
     "iopub.status.busy": "2025-07-16T13:59:19.979057Z",
     "iopub.status.idle": "2025-07-16T13:59:19.996949Z",
     "shell.execute_reply": "2025-07-16T13:59:19.996255Z",
     "shell.execute_reply.started": "2025-07-16T13:59:19.979388Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start chat\n",
    "history = InMemoryChatMessageHistory() #reset chat history\n",
    "chat = ChatUX(wrapped_chain)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:40:20.373685Z",
     "iopub.status.busy": "2025-07-16T13:40:20.373328Z",
     "iopub.status.idle": "2025-07-16T13:40:20.378087Z",
     "shell.execute_reply": "2025-07-16T13:40:20.376917Z",
     "shell.execute_reply.started": "2025-07-16T13:40:20.373662Z"
    }
   },
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tarefa 4.4: Chatbot com persona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta tarefa, o assistente de Inteligência Artificial (IA) desempenha a função de coach de carreira. Você pode usar uma mensagem do sistema para informar ao chatbot qual persona (ou função) ele vai desempenhar. Continue aproveitando a classe InMemoryChatMessageHistory para manter o contexto da conversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:40:22.661761Z",
     "iopub.status.busy": "2025-07-16T13:40:22.661380Z",
     "iopub.status.idle": "2025-07-16T13:40:25.801439Z",
     "shell.execute_reply": "2025-07-16T13:40:25.800630Z",
     "shell.execute_reply.started": "2025-07-16T13:40:22.661728Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \" You will be acting as a career coach. Your goal is to give career advice to users. For questions that are not career related, don't provide advice. Say, I don't know.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history = InMemoryChatMessageHistory() # reset history\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "wrapped_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_history,\n",
    "    history_messages_key=\"career_chat_history\",\n",
    ")\n",
    "\n",
    "response=wrapped_chain.invoke({\"input\": \"What are the career options in AI?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:40:25.803586Z",
     "iopub.status.busy": "2025-07-16T13:40:25.803303Z",
     "iopub.status.idle": "2025-07-16T13:40:26.295070Z",
     "shell.execute_reply": "2025-07-16T13:40:26.294258Z",
     "shell.execute_reply.started": "2025-07-16T13:40:25.803564Z"
    }
   },
   "outputs": [],
   "source": [
    "response=wrapped_chain.invoke({\"input\": \"How to fix my car?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:40:30.185455Z",
     "iopub.status.busy": "2025-07-16T13:40:30.185188Z",
     "iopub.status.idle": "2025-07-16T13:40:30.189775Z",
     "shell.execute_reply": "2025-07-16T13:40:30.188988Z",
     "shell.execute_reply.started": "2025-07-16T13:40:30.185435Z"
    }
   },
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, faça uma pergunta que não esteja dentro da especialidade dessa persona. O modelo não deve responder a essa pergunta e deve dar uma razão para isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 4.5 Chatbot com contexto\n",
    "\n",
    "Nesta tarefa, você pedirá que o chatbot responda perguntas com base no contexto que foi passado para ele. Você usa um arquivo CSV e o modelo do Titan Embeddings para criar um vetor representando esse contexto. Esse vetor é armazenado no Facebook AI Similarity Search (FAISS). Quando uma pergunta for feita ao chatbot, você transmitirá esse vetor de volta para o chatbot e fará com que ele recupere a resposta usando o vetor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geração aumentada de recuperação (RAG)\n",
    "\n",
    "Os chatbots tradicionais estão limitados ao conhecimento contido em seus dados de treinamento. O RAG supera essa limitação:\n",
    "\n",
    "1. **Criando incorporações vetoriais** de seus documentos personalizados\n",
    "2. **Armazenando esses vetores** em um banco de dados pesquisável (como o FAISS)\n",
    "3. **Recuperando o contexto relevante** ao responder perguntas\n",
    "4. **Aumento do prompt do LLM** com esse contexto adicional\n",
    "\n",
    "Essa abordagem permite que o chatbot aproveite seu conhecimento treinado e suas fontes de informações específicas ao gerar respostas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo do Titan Embeddings\n",
    "\n",
    "As incorporações representam palavras, frases ou qualquer outro item distinto como vetores em um espaço de vetores contínuo. Assim, os modelos de machine learning podem fazer operações matemáticas nessas representações e capturar as relações semânticas entre elas.\n",
    "\n",
    "Você usa incorporações para a geração aumentada de recuperação (RAG) [recurso de pesquisa de documentos](https://labelbox.com/blog/how-vector-similarity-search-works/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:57:31.442396Z",
     "iopub.status.busy": "2025-07-16T13:57:31.441775Z",
     "iopub.status.idle": "2025-07-16T13:57:31.446924Z",
     "shell.execute_reply": "2025-07-16T13:57:31.445724Z",
     "shell.execute_reply.started": "2025-07-16T13:57:31.442366Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model configuration\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "br_embeddings = ResourceOptimizedEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v1\",\n",
    "    client=bedrock_client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS como VectorStore\n",
    "\n",
    "Para usar incorporações na pesquisa, você precisa de um armazenamento que faça pesquisas por similaridade de vetores de forma eficaz. Neste caderno, você usa o FAISS, que é um armazenamento em memória. Para armazenar vetores permanentemente, você pode usar as Bases de Conhecimento para Amazon Bedrock, pgVector, Pinecone, Weaviate ou Chroma.\n",
    "\n",
    "As APIs VectorStore do LangChain estão disponíveis [aqui](https://python.langchain.com/v0.2/docs/integrations/vectorstores/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:57:33.547932Z",
     "iopub.status.busy": "2025-07-16T13:57:33.547660Z",
     "iopub.status.idle": "2025-07-16T13:57:49.817000Z",
     "shell.execute_reply": "2025-07-16T13:57:49.816423Z",
     "shell.execute_reply.started": "2025-07-16T13:57:33.547912Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vector store\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "loader = CSVLoader(\"../rag_data/Amazon_SageMaker_FAQs.csv\") # --- > 219 docs with 400 chars\n",
    "documents_aws = loader.load() #\n",
    "print(f\"documents:loaded:size={len(documents_aws)}\")\n",
    "\n",
    "docs = CharacterTextSplitter(chunk_size=2000, chunk_overlap=400, separator=\",\").split_documents(documents_aws)\n",
    "\n",
    "print(f\"Documents:after split and chunking size={len(docs)}\")\n",
    "vectorstore_faiss_aws = None\n",
    "try:\n",
    "    \n",
    "    vectorstore_faiss_aws = FAISS.from_documents(\n",
    "        documents=docs,\n",
    "        embedding = br_embeddings, \n",
    "        #**k_args\n",
    "    )\n",
    "\n",
    "    print(f\"vectorstore_faiss_aws:created={vectorstore_faiss_aws}::\")\n",
    "\n",
    "except ValueError as error:\n",
    "    if  \"AccessDeniedException\" in str(error):\n",
    "        print(f\"\\x1b[41m{error}\\\n",
    "        \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "         \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "         \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")      \n",
    "        class StopExecution(ValueError):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "        raise StopExecution        \n",
    "    else:\n",
    "        raise error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazer um teste rápido com pouco código \n",
    "\n",
    "Você pode usar uma classe wrapper fornecida pelo LangChain para consultar o armazenamento do banco de dados de vetores e retornar os documentos relevantes. Isso processa uma cadeia de QA com todos os valores padrão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:57:49.818711Z",
     "iopub.status.busy": "2025-07-16T13:57:49.817946Z",
     "iopub.status.idle": "2025-07-16T13:57:50.645015Z",
     "shell.execute_reply": "2025-07-16T13:57:50.644068Z",
     "shell.execute_reply.started": "2025-07-16T13:57:49.818684Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_llm = ResourceOptimizedChatBedrock(\n",
    "    model_id=\"amazon.nova-lite-v1:0\",\n",
    "    client=bedrock_client\n",
    ")\n",
    "\n",
    "# wrapper store faiss\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss_aws)\n",
    "print(wrapper_store_faiss.query(\"R in SageMaker\", llm=chat_llm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação de chatbot\n",
    "\n",
    "Para o chatbot, você precisa de gerenciamento de contexto, histórico, armazenamentos de vetores e muitos outros componentes. Comece criando uma cadeia de Geração aumentada de recuperação (RAG) com suporte a contexto.\n",
    "\n",
    "Ela usa as funções **create_stuff_documents_chain** e **create_retrieval_chain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parâmetros e funções usados para a RAG\n",
    "\n",
    "- **Recuperador:** você usou `VectorStoreRetriever`, que é apoiado por um `VectorStore`. Para recuperar texto, há dois tipos de pesquisa para escolher: `\"similarity\"` ou `\"mmr\"`. `search_type=\"similarity\"` usa a pesquisa por similaridade no objeto recuperador, onde seleciona vetores de fragmentos de texto que são mais semelhantes ao vetor da pergunta.\n",
    "\n",
    "- **create_stuff_documents_chain** especifica como o contexto recuperado é alimentado em um prompt e no LLM. Os documentos recuperados são \"preenchidos\" como contexto sem qualquer resumo ou outro processamento no prompt.\n",
    "\n",
    "- **create_retrieval_chain** adiciona a etapa de recuperação, propaga o contexto recuperado pela cadeia e o fornece junto com a resposta final. \n",
    "\n",
    "Se a pergunta feita estiver fora do escopo do contexto, o modelo responderá que não sabe a resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:57:50.646837Z",
     "iopub.status.busy": "2025-07-16T13:57:50.646385Z",
     "iopub.status.idle": "2025-07-16T13:57:51.145258Z",
     "shell.execute_reply": "2025-07-16T13:57:51.144419Z",
     "shell.execute_reply.started": "2025-07-16T13:57:50.646814Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever=vectorstore_faiss_aws.as_retriever()\n",
    "question_answer_chain = create_stuff_documents_chain(chat_llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"What is sagemaker?\"})\n",
    "print(response) # shows the document chunks consulted to come up with the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, inicie uma conversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T13:57:51.147642Z",
     "iopub.status.busy": "2025-07-16T13:57:51.147356Z",
     "iopub.status.idle": "2025-07-16T13:57:51.165562Z",
     "shell.execute_reply": "2025-07-16T13:57:51.164835Z",
     "shell.execute_reply.started": "2025-07-16T13:57:51.147618Z"
    }
   },
   "outputs": [],
   "source": [
    "chat = ChatUX(rag_chain, retrievalChain=True)\n",
    "chat.start_chat()  # Only answers will be shown here, and not the citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Você utilizou os LLMs Nova Lite e Titan para desenvolver uma interface conversacional seguindo os seguintes padrões:\n",
    "\n",
    "- Chatbot (básico: sem contexto)\n",
    "- Chatbot usando modelo de prompt (LangChain)\n",
    "- Chatbot com personas\n",
    "- Chatbot com contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimente você mesmo\n",
    "\n",
    "- Altere os prompts para seu caso de uso específico e avalie o resultado de diferentes modelos.\n",
    "- Teste o comprimento do token para entender a latência e a responsividade do serviço.\n",
    "- Aplique diferentes princípios de engenharia de prompts para gerar resultados melhores.\n",
    "\n",
    "### Limpeza\n",
    "\n",
    "Você concluiu este caderno. Passe para a próxima parte do laboratório da seguinte forma:\n",
    "\n",
    "- Feche este arquivo de caderno e continue com a **Tarefa 5**."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}